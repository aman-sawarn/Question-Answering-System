{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 QA system using keras-BABI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KbKitieRmDV",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/keras-team/keras/blob/master/examples/babi_rnn.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq9bko9XRaAa",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/keras-team/keras/blob/master/examples/babi_memnn.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltxOD8PARHf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
        "from keras.layers import add, dot, concatenate\n",
        "from keras.layers import LSTM\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from functools import reduce\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "def tokenize(sent):\n",
        "    '''Return the tokens of a sentence including punctuation.\n",
        "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
        "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
        "    '''\n",
        "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]\n",
        "\n",
        "\n",
        "def parse_stories(lines, only_supporting=False):\n",
        "    '''Parse stories provided in the bAbi tasks format\n",
        "    If only_supporting is true, only the sentences\n",
        "    that support the answer are kept.\n",
        "    '''\n",
        "    data = []\n",
        "    story = []\n",
        "    for line in lines:\n",
        "        line = line.decode('utf-8').strip()\n",
        "        nid, line = line.split(' ', 1)\n",
        "        nid = int(nid)\n",
        "        if nid == 1:\n",
        "            story = []\n",
        "        if '\\t' in line:\n",
        "            q, a, supporting = line.split('\\t')\n",
        "            q = tokenize(q)\n",
        "            if only_supporting:\n",
        "                # Only select the related substory\n",
        "                supporting = map(int, supporting.split())\n",
        "                substory = [story[i - 1] for i in supporting]\n",
        "            else:\n",
        "                # Provide all the substories\n",
        "                substory = [x for x in story if x]\n",
        "            data.append((substory, q, a))\n",
        "            story.append('')\n",
        "        else:\n",
        "            sent = tokenize(line)\n",
        "            story.append(sent)\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_stories(f, only_supporting=False, max_length=None):\n",
        "    '''Given a file name, read the file,\n",
        "    retrieve the stories,\n",
        "    and then convert the sentences into a single story.\n",
        "    If max_length is supplied,\n",
        "    any stories longer than max_length tokens will be discarded.\n",
        "    '''\n",
        "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
        "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
        "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
        "            if not max_length or len(flatten(story)) < max_length]\n",
        "    return data\n",
        "\n",
        "\n",
        "def vectorize_stories(data):\n",
        "    inputs, queries, answers = [], [], []\n",
        "    for story, query, answer in data:\n",
        "        inputs.append([word_idx[w] for w in story])\n",
        "        queries.append([word_idx[w] for w in query])\n",
        "        answers.append(word_idx[answer])\n",
        "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
        "            pad_sequences(queries, maxlen=query_maxlen),\n",
        "            np.array(answers))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwhMprBSiAMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dae8ce07-1eed-4740-9d53-63b11827ba9d"
      },
      "source": [
        "try:\n",
        "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
        "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
        "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
        "except:\n",
        "    print('Error downloading dataset, please download it manually:\\n'\n",
        "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
        "          '.tar.gz\\n'\n",
        "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
        "    raise\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
            "11747328/11745123 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKCz3A1eiARS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "73e3d76b-4ae7-4592-c921-a64cba3ce41b"
      },
      "source": [
        "challenges = {\n",
        "    # QA1 with 10,000 samples\n",
        "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_'\n",
        "                                  'single-supporting-fact_{}.txt',\n",
        "    # QA2 with 10,000 samples\n",
        "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_'\n",
        "                                'two-supporting-facts_{}.txt',\n",
        "}\n",
        "challenge_type = 'single_supporting_fact_10k'\n",
        "challenge = challenges[challenge_type]\n",
        "\n",
        "print('Extracting stories for the challenge:', challenge_type)\n",
        "with tarfile.open(path) as tar:\n",
        "    train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
        "    test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
        "\n",
        "vocab = set()\n",
        "for story, q, answer in train_stories + test_stories:\n",
        "    vocab |= set(story + q + [answer])\n",
        "vocab = sorted(vocab)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting stories for the challenge: single_supporting_fact_10k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return _compile(pattern, flags).split(string, maxsplit)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTAXXGUiiAWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "02f5cc62-cae1-44af-d1fa-5b8be0ffa9d3"
      },
      "source": [
        "# Reserve 0 for masking via pad_sequences\n",
        "vocab_size = len(vocab) + 1\n",
        "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
        "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
        "\n",
        "print('-')\n",
        "print('Vocab size:', vocab_size, 'unique words')\n",
        "print('Story max length:', story_maxlen, 'words')\n",
        "print('Query max length:', query_maxlen, 'words')\n",
        "print('Number of training stories:', len(train_stories))\n",
        "print('Number of test stories:', len(test_stories))\n",
        "print('-')\n",
        "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
        "print(train_stories[0])\n",
        "print('-')\n",
        "print('Vectorizing the word sequences...')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Vocab size: 22 unique words\n",
            "Story max length: 68 words\n",
            "Query max length: 4 words\n",
            "Number of training stories: 10000\n",
            "Number of test stories: 1000\n",
            "-\n",
            "Here's what a \"story\" tuple looks like (input, query, answer):\n",
            "(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'John', 'went', 'to', 'the', 'hallway', '.'], ['Where', 'is', 'Mary', '?'], 'bathroom')\n",
            "-\n",
            "Vectorizing the word sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VijfrlxNiAb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "5b222a0f-0641-4e67-c556-4216c540f7e7"
      },
      "source": [
        "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
        "inputs_train, queries_train, answers_train = vectorize_stories(train_stories)\n",
        "inputs_test, queries_test, answers_test = vectorize_stories(test_stories)\n",
        "\n",
        "print('-')\n",
        "print('inputs: integer tensor of shape (samples, max_length)')\n",
        "print('inputs_train shape:', inputs_train.shape)\n",
        "print('inputs_test shape:', inputs_test.shape)\n",
        "print('-')\n",
        "print('queries: integer tensor of shape (samples, max_length)')\n",
        "print('queries_train shape:', queries_train.shape)\n",
        "print('queries_test shape:', queries_test.shape)\n",
        "print('-')\n",
        "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
        "print('answers_train shape:', answers_train.shape)\n",
        "print('answers_test shape:', answers_test.shape)\n",
        "print('-')\n",
        "print('Compiling...')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "inputs: integer tensor of shape (samples, max_length)\n",
            "inputs_train shape: (10000, 68)\n",
            "inputs_test shape: (1000, 68)\n",
            "-\n",
            "queries: integer tensor of shape (samples, max_length)\n",
            "queries_train shape: (10000, 4)\n",
            "queries_test shape: (1000, 4)\n",
            "-\n",
            "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
            "answers_train shape: (10000,)\n",
            "answers_test shape: (1000,)\n",
            "-\n",
            "Compiling...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsu792UfiAbU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e03420e5-524f-4d5a-fc94-f270492bec41"
      },
      "source": [
        "# placeholders\n",
        "input_sequence = Input((story_maxlen,))\n",
        "question = Input((query_maxlen,))\n",
        "\n",
        "# encoders\n",
        "# embed the input sequence into a sequence of vectors\n",
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
        "                              output_dim=64))\n",
        "input_encoder_m.add(Dropout(0.3))\n",
        "# output: (samples, story_maxlen, embedding_dim)\n",
        "\n",
        "# embed the input into a sequence of vectors of size query_maxlen\n",
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
        "                              output_dim=query_maxlen))\n",
        "input_encoder_c.add(Dropout(0.3))\n",
        "# output: (samples, story_maxlen, query_maxlen)\n",
        "\n",
        "# embed the question into a sequence of vectors\n",
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim=vocab_size,\n",
        "                               output_dim=64,\n",
        "                               input_length=query_maxlen))\n",
        "question_encoder.add(Dropout(0.3))\n",
        "# output: (samples, query_maxlen, embedding_dim)\n",
        "\n",
        "# encode input sequence and questions (which are indices)\n",
        "# to sequences of dense vectors\n",
        "input_encoded_m = input_encoder_m(input_sequence)\n",
        "input_encoded_c = input_encoder_c(input_sequence)\n",
        "question_encoded = question_encoder(question)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp1ocZ4biAVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f694e7fa-c199-4741-f9ef-d2d7085292f4"
      },
      "source": [
        "# compute a 'match' between the first input vector sequence\n",
        "# and the question vector sequence\n",
        "# shape: `(samples, story_maxlen, query_maxlen)`\n",
        "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
        "match = Activation('softmax')(match)\n",
        "\n",
        "# add the match matrix with the second input vector sequence\n",
        "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
        "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
        "\n",
        "# concatenate the match matrix with the question vector sequence\n",
        "answer = concatenate([response, question_encoded])\n",
        "\n",
        "# the original paper uses a matrix multiplication for this reduction step.\n",
        "# we choose to use a RNN instead.\n",
        "answer = LSTM(32)(answer)  # (samples, 32)\n",
        "\n",
        "# one regularization layer -- more would probably be needed.\n",
        "answer = Dropout(0.3)(answer)\n",
        "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
        "# we output a probability distribution over the vocabulary\n",
        "answer = Activation('softmax')(answer)\n",
        "\n",
        "# build the final model\n",
        "model = Model([input_sequence, question], answer)\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFRq-SpqiAQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "307cc6d1-141c-4ff3-ef58-305cab3f887d"
      },
      "source": [
        "# train\n",
        "history=model.fit([inputs_train, queries_train], answers_train,\n",
        "          batch_size=32,\n",
        "          epochs=120,\n",
        "          validation_data=([inputs_test, queries_test], answers_test))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/120\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.4008 - acc: 0.4714 - val_loss: 1.3747 - val_acc: 0.4740\n",
            "Epoch 2/120\n",
            "10000/10000 [==============================] - 5s 478us/step - loss: 1.3721 - acc: 0.4757 - val_loss: 1.3782 - val_acc: 0.4830\n",
            "Epoch 3/120\n",
            "10000/10000 [==============================] - 5s 488us/step - loss: 1.3606 - acc: 0.4750 - val_loss: 1.3510 - val_acc: 0.4880\n",
            "Epoch 4/120\n",
            "10000/10000 [==============================] - 5s 483us/step - loss: 1.3527 - acc: 0.4793 - val_loss: 1.3602 - val_acc: 0.4680\n",
            "Epoch 5/120\n",
            "10000/10000 [==============================] - 5s 477us/step - loss: 1.3396 - acc: 0.4871 - val_loss: 1.3370 - val_acc: 0.4820\n",
            "Epoch 6/120\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.3355 - acc: 0.4870 - val_loss: 1.3560 - val_acc: 0.4550\n",
            "Epoch 7/120\n",
            "10000/10000 [==============================] - 5s 476us/step - loss: 1.3337 - acc: 0.4883 - val_loss: 1.3180 - val_acc: 0.4980\n",
            "Epoch 8/120\n",
            "10000/10000 [==============================] - 5s 500us/step - loss: 1.3083 - acc: 0.5043 - val_loss: 1.2979 - val_acc: 0.5140\n",
            "Epoch 9/120\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.2861 - acc: 0.5042 - val_loss: 1.2774 - val_acc: 0.5000\n",
            "Epoch 10/120\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.2598 - acc: 0.5069 - val_loss: 1.2577 - val_acc: 0.5030\n",
            "Epoch 11/120\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 1.2432 - acc: 0.5108 - val_loss: 1.2245 - val_acc: 0.5100\n",
            "Epoch 12/120\n",
            "10000/10000 [==============================] - 5s 472us/step - loss: 1.2281 - acc: 0.5131 - val_loss: 1.2228 - val_acc: 0.5100\n",
            "Epoch 13/120\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.2111 - acc: 0.5172 - val_loss: 1.2002 - val_acc: 0.5270\n",
            "Epoch 14/120\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.2103 - acc: 0.5153 - val_loss: 1.1968 - val_acc: 0.5240\n",
            "Epoch 15/120\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.2101 - acc: 0.5107 - val_loss: 1.1899 - val_acc: 0.5140\n",
            "Epoch 16/120\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.1972 - acc: 0.5193 - val_loss: 1.1818 - val_acc: 0.5200\n",
            "Epoch 17/120\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.1999 - acc: 0.5157 - val_loss: 1.1949 - val_acc: 0.5130\n",
            "Epoch 18/120\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.1841 - acc: 0.5230 - val_loss: 1.1815 - val_acc: 0.5170\n",
            "Epoch 19/120\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.1803 - acc: 0.5214 - val_loss: 1.1875 - val_acc: 0.5050\n",
            "Epoch 20/120\n",
            "10000/10000 [==============================] - 5s 485us/step - loss: 1.1776 - acc: 0.5233 - val_loss: 1.1771 - val_acc: 0.5240\n",
            "Epoch 21/120\n",
            "10000/10000 [==============================] - 5s 473us/step - loss: 1.1701 - acc: 0.5235 - val_loss: 1.1669 - val_acc: 0.5220\n",
            "Epoch 22/120\n",
            "10000/10000 [==============================] - 5s 467us/step - loss: 1.1627 - acc: 0.5244 - val_loss: 1.1734 - val_acc: 0.5150\n",
            "Epoch 23/120\n",
            "10000/10000 [==============================] - 5s 484us/step - loss: 1.1594 - acc: 0.5234 - val_loss: 1.1601 - val_acc: 0.5200\n",
            "Epoch 24/120\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.1565 - acc: 0.5314 - val_loss: 1.1519 - val_acc: 0.5210\n",
            "Epoch 25/120\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.1542 - acc: 0.5281 - val_loss: 1.1547 - val_acc: 0.5180\n",
            "Epoch 26/120\n",
            "10000/10000 [==============================] - 5s 481us/step - loss: 1.1473 - acc: 0.5230 - val_loss: 1.1593 - val_acc: 0.5290\n",
            "Epoch 27/120\n",
            "10000/10000 [==============================] - 5s 491us/step - loss: 1.1464 - acc: 0.5272 - val_loss: 1.1484 - val_acc: 0.5190\n",
            "Epoch 28/120\n",
            "10000/10000 [==============================] - 5s 491us/step - loss: 1.1288 - acc: 0.5342 - val_loss: 1.1490 - val_acc: 0.5130\n",
            "Epoch 29/120\n",
            "10000/10000 [==============================] - 5s 468us/step - loss: 1.1237 - acc: 0.5310 - val_loss: 1.1536 - val_acc: 0.5240\n",
            "Epoch 30/120\n",
            "10000/10000 [==============================] - 5s 482us/step - loss: 1.1236 - acc: 0.5321 - val_loss: 1.1869 - val_acc: 0.4940\n",
            "Epoch 31/120\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.1260 - acc: 0.5403 - val_loss: 1.1299 - val_acc: 0.5300\n",
            "Epoch 32/120\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 1.1192 - acc: 0.5359 - val_loss: 1.1502 - val_acc: 0.5140\n",
            "Epoch 33/120\n",
            "10000/10000 [==============================] - 5s 508us/step - loss: 1.1129 - acc: 0.5442 - val_loss: 1.1563 - val_acc: 0.5190\n",
            "Epoch 34/120\n",
            "10000/10000 [==============================] - 5s 486us/step - loss: 1.0912 - acc: 0.5494 - val_loss: 1.1285 - val_acc: 0.5370\n",
            "Epoch 35/120\n",
            "10000/10000 [==============================] - 5s 480us/step - loss: 1.0711 - acc: 0.5606 - val_loss: 1.0971 - val_acc: 0.5630\n",
            "Epoch 36/120\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.0292 - acc: 0.5969 - val_loss: 1.0137 - val_acc: 0.6000\n",
            "Epoch 37/120\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 0.9317 - acc: 0.6468 - val_loss: 0.8946 - val_acc: 0.6750\n",
            "Epoch 38/120\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 0.8171 - acc: 0.6993 - val_loss: 0.8270 - val_acc: 0.7090\n",
            "Epoch 39/120\n",
            "10000/10000 [==============================] - 5s 471us/step - loss: 0.7355 - acc: 0.7352 - val_loss: 0.7119 - val_acc: 0.7330\n",
            "Epoch 40/120\n",
            "10000/10000 [==============================] - 5s 485us/step - loss: 0.6783 - acc: 0.7555 - val_loss: 0.7033 - val_acc: 0.7340\n",
            "Epoch 41/120\n",
            "10000/10000 [==============================] - 5s 482us/step - loss: 0.6459 - acc: 0.7648 - val_loss: 0.6391 - val_acc: 0.7470\n",
            "Epoch 42/120\n",
            "10000/10000 [==============================] - 5s 482us/step - loss: 0.6133 - acc: 0.7749 - val_loss: 0.6169 - val_acc: 0.7680\n",
            "Epoch 43/120\n",
            "10000/10000 [==============================] - 5s 484us/step - loss: 0.5899 - acc: 0.7804 - val_loss: 0.5932 - val_acc: 0.7620\n",
            "Epoch 44/120\n",
            "10000/10000 [==============================] - 5s 480us/step - loss: 0.5598 - acc: 0.7912 - val_loss: 0.5582 - val_acc: 0.7830\n",
            "Epoch 45/120\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 0.5262 - acc: 0.8031 - val_loss: 0.5312 - val_acc: 0.7920\n",
            "Epoch 46/120\n",
            "10000/10000 [==============================] - 5s 473us/step - loss: 0.4924 - acc: 0.8178 - val_loss: 0.4997 - val_acc: 0.7990\n",
            "Epoch 47/120\n",
            "10000/10000 [==============================] - 5s 480us/step - loss: 0.4607 - acc: 0.8295 - val_loss: 0.4519 - val_acc: 0.8260\n",
            "Epoch 48/120\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.4451 - acc: 0.8306 - val_loss: 0.4274 - val_acc: 0.8360\n",
            "Epoch 49/120\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.4075 - acc: 0.8444 - val_loss: 0.4086 - val_acc: 0.8380\n",
            "Epoch 50/120\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 0.3939 - acc: 0.8512 - val_loss: 0.4093 - val_acc: 0.8430\n",
            "Epoch 51/120\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 0.3778 - acc: 0.8611 - val_loss: 0.4058 - val_acc: 0.8410\n",
            "Epoch 52/120\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 0.3627 - acc: 0.8653 - val_loss: 0.3734 - val_acc: 0.8580\n",
            "Epoch 53/120\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 0.3446 - acc: 0.8746 - val_loss: 0.3744 - val_acc: 0.8570\n",
            "Epoch 54/120\n",
            "10000/10000 [==============================] - 5s 488us/step - loss: 0.3417 - acc: 0.8729 - val_loss: 0.4441 - val_acc: 0.8310\n",
            "Epoch 55/120\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 0.3237 - acc: 0.8782 - val_loss: 0.3629 - val_acc: 0.8580\n",
            "Epoch 56/120\n",
            "10000/10000 [==============================] - 5s 518us/step - loss: 0.3236 - acc: 0.8786 - val_loss: 0.3473 - val_acc: 0.8660\n",
            "Epoch 57/120\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 0.3098 - acc: 0.8796 - val_loss: 0.3514 - val_acc: 0.8670\n",
            "Epoch 58/120\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.2995 - acc: 0.8864 - val_loss: 0.3254 - val_acc: 0.8840\n",
            "Epoch 59/120\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 0.2951 - acc: 0.8878 - val_loss: 0.3178 - val_acc: 0.8800\n",
            "Epoch 60/120\n",
            "10000/10000 [==============================] - 5s 474us/step - loss: 0.2786 - acc: 0.8952 - val_loss: 0.3088 - val_acc: 0.8870\n",
            "Epoch 61/120\n",
            "10000/10000 [==============================] - 5s 494us/step - loss: 0.2772 - acc: 0.8942 - val_loss: 0.3191 - val_acc: 0.8840\n",
            "Epoch 62/120\n",
            "10000/10000 [==============================] - 5s 486us/step - loss: 0.2653 - acc: 0.9020 - val_loss: 0.3234 - val_acc: 0.8780\n",
            "Epoch 63/120\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.2612 - acc: 0.9019 - val_loss: 0.2892 - val_acc: 0.8920\n",
            "Epoch 64/120\n",
            "10000/10000 [==============================] - 5s 496us/step - loss: 0.2516 - acc: 0.9075 - val_loss: 0.2907 - val_acc: 0.8940\n",
            "Epoch 65/120\n",
            "10000/10000 [==============================] - 5s 496us/step - loss: 0.2381 - acc: 0.9108 - val_loss: 0.3150 - val_acc: 0.8780\n",
            "Epoch 66/120\n",
            "10000/10000 [==============================] - 5s 475us/step - loss: 0.2338 - acc: 0.9139 - val_loss: 0.3057 - val_acc: 0.8860\n",
            "Epoch 67/120\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 0.2310 - acc: 0.9141 - val_loss: 0.2597 - val_acc: 0.9030\n",
            "Epoch 68/120\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 0.2187 - acc: 0.9237 - val_loss: 0.2577 - val_acc: 0.8940\n",
            "Epoch 69/120\n",
            "10000/10000 [==============================] - 5s 486us/step - loss: 0.2043 - acc: 0.9263 - val_loss: 0.2464 - val_acc: 0.9000\n",
            "Epoch 70/120\n",
            "10000/10000 [==============================] - 5s 485us/step - loss: 0.1972 - acc: 0.9306 - val_loss: 0.2411 - val_acc: 0.9090\n",
            "Epoch 71/120\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.1919 - acc: 0.9296 - val_loss: 0.2342 - val_acc: 0.9130\n",
            "Epoch 72/120\n",
            "10000/10000 [==============================] - 5s 489us/step - loss: 0.1871 - acc: 0.9355 - val_loss: 0.2187 - val_acc: 0.9200\n",
            "Epoch 73/120\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 0.1820 - acc: 0.9332 - val_loss: 0.2353 - val_acc: 0.9070\n",
            "Epoch 74/120\n",
            "10000/10000 [==============================] - 5s 476us/step - loss: 0.1789 - acc: 0.9354 - val_loss: 0.2131 - val_acc: 0.9160\n",
            "Epoch 75/120\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 0.1689 - acc: 0.9404 - val_loss: 0.1950 - val_acc: 0.9270\n",
            "Epoch 76/120\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 0.1604 - acc: 0.9416 - val_loss: 0.2310 - val_acc: 0.9160\n",
            "Epoch 77/120\n",
            "10000/10000 [==============================] - 5s 476us/step - loss: 0.1649 - acc: 0.9414 - val_loss: 0.2150 - val_acc: 0.9250\n",
            "Epoch 78/120\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 0.1561 - acc: 0.9458 - val_loss: 0.2027 - val_acc: 0.9210\n",
            "Epoch 79/120\n",
            "10000/10000 [==============================] - 5s 471us/step - loss: 0.1541 - acc: 0.9462 - val_loss: 0.2078 - val_acc: 0.9270\n",
            "Epoch 80/120\n",
            "10000/10000 [==============================] - 5s 477us/step - loss: 0.1415 - acc: 0.9492 - val_loss: 0.1858 - val_acc: 0.9280\n",
            "Epoch 81/120\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 0.1398 - acc: 0.9491 - val_loss: 0.1809 - val_acc: 0.9310\n",
            "Epoch 82/120\n",
            "10000/10000 [==============================] - 5s 482us/step - loss: 0.1326 - acc: 0.9515 - val_loss: 0.2118 - val_acc: 0.9270\n",
            "Epoch 83/120\n",
            "10000/10000 [==============================] - 5s 474us/step - loss: 0.1352 - acc: 0.9551 - val_loss: 0.1869 - val_acc: 0.9350\n",
            "Epoch 84/120\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 0.1319 - acc: 0.9548 - val_loss: 0.1843 - val_acc: 0.9320\n",
            "Epoch 85/120\n",
            "10000/10000 [==============================] - 5s 491us/step - loss: 0.1313 - acc: 0.9541 - val_loss: 0.1840 - val_acc: 0.9380\n",
            "Epoch 86/120\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.1153 - acc: 0.9597 - val_loss: 0.1721 - val_acc: 0.9350\n",
            "Epoch 87/120\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 0.1213 - acc: 0.9580 - val_loss: 0.1599 - val_acc: 0.9420\n",
            "Epoch 88/120\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 0.1113 - acc: 0.9589 - val_loss: 0.1623 - val_acc: 0.9410\n",
            "Epoch 89/120\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 0.1061 - acc: 0.9642 - val_loss: 0.1824 - val_acc: 0.9340\n",
            "Epoch 90/120\n",
            "10000/10000 [==============================] - 5s 481us/step - loss: 0.1082 - acc: 0.9603 - val_loss: 0.1744 - val_acc: 0.9400\n",
            "Epoch 91/120\n",
            "10000/10000 [==============================] - 5s 512us/step - loss: 0.1037 - acc: 0.9635 - val_loss: 0.1840 - val_acc: 0.9310\n",
            "Epoch 92/120\n",
            "10000/10000 [==============================] - 5s 476us/step - loss: 0.1041 - acc: 0.9645 - val_loss: 0.1436 - val_acc: 0.9450\n",
            "Epoch 93/120\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 0.1008 - acc: 0.9640 - val_loss: 0.1379 - val_acc: 0.9480\n",
            "Epoch 94/120\n",
            "10000/10000 [==============================] - 5s 478us/step - loss: 0.0962 - acc: 0.9673 - val_loss: 0.1472 - val_acc: 0.9490\n",
            "Epoch 95/120\n",
            "10000/10000 [==============================] - 5s 483us/step - loss: 0.1010 - acc: 0.9637 - val_loss: 0.1637 - val_acc: 0.9410\n",
            "Epoch 96/120\n",
            "10000/10000 [==============================] - 5s 471us/step - loss: 0.0955 - acc: 0.9685 - val_loss: 0.1866 - val_acc: 0.9360\n",
            "Epoch 97/120\n",
            "10000/10000 [==============================] - 5s 483us/step - loss: 0.0882 - acc: 0.9704 - val_loss: 0.1528 - val_acc: 0.9480\n",
            "Epoch 98/120\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 0.0898 - acc: 0.9708 - val_loss: 0.1614 - val_acc: 0.9440\n",
            "Epoch 99/120\n",
            "10000/10000 [==============================] - 5s 475us/step - loss: 0.0818 - acc: 0.9720 - val_loss: 0.1543 - val_acc: 0.9480\n",
            "Epoch 100/120\n",
            "10000/10000 [==============================] - 5s 484us/step - loss: 0.0839 - acc: 0.9709 - val_loss: 0.1554 - val_acc: 0.9430\n",
            "Epoch 101/120\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 0.0843 - acc: 0.9707 - val_loss: 0.1387 - val_acc: 0.9510\n",
            "Epoch 102/120\n",
            "10000/10000 [==============================] - 5s 485us/step - loss: 0.0832 - acc: 0.9703 - val_loss: 0.1433 - val_acc: 0.9490\n",
            "Epoch 103/120\n",
            "10000/10000 [==============================] - 5s 474us/step - loss: 0.0781 - acc: 0.9730 - val_loss: 0.1806 - val_acc: 0.9450\n",
            "Epoch 104/120\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 0.0746 - acc: 0.9754 - val_loss: 0.1849 - val_acc: 0.9400\n",
            "Epoch 105/120\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 0.0748 - acc: 0.9766 - val_loss: 0.1346 - val_acc: 0.9520\n",
            "Epoch 106/120\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 0.0742 - acc: 0.9751 - val_loss: 0.1511 - val_acc: 0.9510\n",
            "Epoch 107/120\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 0.0749 - acc: 0.9749 - val_loss: 0.1510 - val_acc: 0.9510\n",
            "Epoch 108/120\n",
            "10000/10000 [==============================] - 5s 476us/step - loss: 0.0709 - acc: 0.9769 - val_loss: 0.1542 - val_acc: 0.9500\n",
            "Epoch 109/120\n",
            "10000/10000 [==============================] - 5s 477us/step - loss: 0.0728 - acc: 0.9773 - val_loss: 0.1274 - val_acc: 0.9520\n",
            "Epoch 110/120\n",
            "10000/10000 [==============================] - 5s 474us/step - loss: 0.0669 - acc: 0.9783 - val_loss: 0.1342 - val_acc: 0.9570\n",
            "Epoch 111/120\n",
            "10000/10000 [==============================] - 5s 488us/step - loss: 0.0631 - acc: 0.9776 - val_loss: 0.1350 - val_acc: 0.9540\n",
            "Epoch 112/120\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.0633 - acc: 0.9783 - val_loss: 0.1405 - val_acc: 0.9550\n",
            "Epoch 113/120\n",
            "10000/10000 [==============================] - 5s 486us/step - loss: 0.0629 - acc: 0.9800 - val_loss: 0.1310 - val_acc: 0.9580\n",
            "Epoch 114/120\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 0.0591 - acc: 0.9809 - val_loss: 0.1426 - val_acc: 0.9540\n",
            "Epoch 115/120\n",
            "10000/10000 [==============================] - 5s 486us/step - loss: 0.0605 - acc: 0.9805 - val_loss: 0.1732 - val_acc: 0.9490\n",
            "Epoch 116/120\n",
            "10000/10000 [==============================] - 5s 471us/step - loss: 0.0567 - acc: 0.9801 - val_loss: 0.1337 - val_acc: 0.9580\n",
            "Epoch 117/120\n",
            "10000/10000 [==============================] - 5s 480us/step - loss: 0.0499 - acc: 0.9830 - val_loss: 0.1169 - val_acc: 0.9570\n",
            "Epoch 118/120\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.0572 - acc: 0.9823 - val_loss: 0.1428 - val_acc: 0.9520\n",
            "Epoch 119/120\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.0542 - acc: 0.9825 - val_loss: 0.1468 - val_acc: 0.9520\n",
            "Epoch 120/120\n",
            "10000/10000 [==============================] - 5s 472us/step - loss: 0.0531 - acc: 0.9829 - val_loss: 0.1258 - val_acc: 0.9650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w2g3OFyiALN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}