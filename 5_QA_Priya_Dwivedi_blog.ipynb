{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5 QA Priya Dwivedi blog.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLdaiEDJZ4qh",
        "colab_type": "code",
        "outputId": "40d3a0b8-237a-42f7-edaa-149903c5c346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import argparse\n",
        "import json\n",
        "import nltk\n",
        "from importlib import reload\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "reload(sys)\n",
        "sys.setdefaultencoding('utf8')\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "SQUAD_BASE_URL = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
        "\n",
        "\n",
        "def setup_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data_dir\", required=True)\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def write_to_file(out_file, line):\n",
        "    out_file.write(line.encode('utf8') + '\\n')\n",
        "\n",
        "\n",
        "def data_from_json(filename):\n",
        "    \"\"\"Loads JSON data from filename and returns\"\"\"\n",
        "    with open(filename) as data_file:\n",
        "        data = json.load(data_file)\n",
        "    return data\n",
        "\n",
        "\n",
        "def tokenize(sequence):\n",
        "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in nltk.word_tokenize(sequence)]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def total_exs(dataset):\n",
        "    \"\"\"\n",
        "    Returns the total number of (context, question, answer) triples,\n",
        "    given the data read from the SQuAD json file.\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    for article in dataset['data']:\n",
        "        for para in article['paragraphs']:\n",
        "            total += len(para['qas'])\n",
        "    return total\n",
        "\n",
        "\n",
        "def reporthook(t):\n",
        "    \"\"\"https://github.com/tqdm/tqdm\"\"\"\n",
        "    last_b = [0]\n",
        "\n",
        "    def inner(b=1, bsize=1, tsize=None):\n",
        "        \"\"\"\n",
        "        b: int, optional\n",
        "            Number of blocks just transferred [default: 1].\n",
        "        bsize: int, optional\n",
        "            Size of each block (in tqdm units) [default: 1].\n",
        "        tsize: int, optional\n",
        "            Total size (in tqdm units). If [default: None] remains unchanged.\n",
        "        \"\"\"\n",
        "        if tsize is not None:\n",
        "            t.total = tsize\n",
        "        t.update((b - last_b[0]) * bsize)\n",
        "        last_b[0] = b\n",
        "\n",
        "    return inner\n",
        "\n",
        "\n",
        "def maybe_download(url, filename, prefix, num_bytes=None):\n",
        "    \"\"\"Takes an URL, a filename, and the expected bytes, download\n",
        "    the contents and returns the filename.\n",
        "    num_bytes=None disables the file size check.\"\"\"\n",
        "    local_filename = None\n",
        "    if not os.path.exists(os.path.join(prefix, filename)):\n",
        "        try:\n",
        "            print(\"Downloading file {}...\".format(url + filename))\n",
        "            with tqdm(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n",
        "                local_filename, _ = urlretrieve(url + filename, os.path.join(prefix, filename), reporthook=reporthook(t))\n",
        "        except AttributeError as e:\n",
        "            print(\"An error occurred when downloading the file! Please get the dataset using a browser.\")\n",
        "            raise e\n",
        "    # We have a downloaded file\n",
        "    # Check the stats and make sure they are ok\n",
        "    file_stats = os.stat(os.path.join(prefix, filename))\n",
        "    if num_bytes is None or file_stats.st_size == num_bytes:\n",
        "        print(\"File {} successfully loaded\".format(filename))\n",
        "    else:\n",
        "        raise Exception(\"Unexpected dataset size. Please get the dataset using a browser.\")\n",
        "\n",
        "    return local_filename\n",
        "\n",
        "\n",
        "\n",
        "def get_char_word_loc_mapping(context, context_tokens):\n",
        "    \"\"\"\n",
        "    Return a mapping that maps from character locations to the corresponding token locations.\n",
        "    If we're unable to complete the mapping e.g. because of special characters, we return None.\n",
        "    Inputs:\n",
        "      context: string (unicode)\n",
        "      context_tokens: list of strings (unicode)\n",
        "    Returns:\n",
        "      mapping: dictionary from ints (character locations) to (token, token_idx) pairs\n",
        "        Only ints corresponding to non-space character locations are in the keys\n",
        "        e.g. if context = \"hello world\" and context_tokens = [\"hello\", \"world\"] then\n",
        "        0,1,2,3,4 are mapped to (\"hello\", 0) and 6,7,8,9,10 are mapped to (\"world\", 1)\n",
        "    \"\"\"\n",
        "    acc = '' # accumulator\n",
        "    current_token_idx = 0 # current word loc\n",
        "    mapping = dict()\n",
        "\n",
        "    for char_idx, char in enumerate(context): # step through original characters\n",
        "        if char != u' ' and char != u'\\n': # if it's not a space:\n",
        "            acc += char # add to accumulator\n",
        "            context_token = unicode(context_tokens[current_token_idx]) # current word token\n",
        "            if acc == context_token: # if the accumulator now matches the current word token\n",
        "                syn_start = char_idx - len(acc) + 1 # char loc of the start of this word\n",
        "                for char_loc in range(syn_start, char_idx+1):\n",
        "                    mapping[char_loc] = (acc, current_token_idx) # add to mapping\n",
        "                acc = '' # reset accumulator\n",
        "                current_token_idx += 1\n",
        "\n",
        "    if current_token_idx != len(context_tokens):\n",
        "        return None\n",
        "    else:\n",
        "        return mapping\n",
        "\n",
        "\n",
        "def preprocess_and_write(dataset, tier, out_dir):\n",
        "    \"\"\"Reads the dataset, extracts context, question, answer, tokenizes them,\n",
        "    and calculates answer span in terms of token indices.\n",
        "    Note: due to tokenization issues, and the fact that the original answer\n",
        "    spans are given in terms of characters, some examples are discarded because\n",
        "    we cannot get a clean span in terms of tokens.\n",
        "    This function produces the {train/dev}.{context/question/answer/span} files.\n",
        "    Inputs:\n",
        "      dataset: read from JSON\n",
        "      tier: string (\"train\" or \"dev\")\n",
        "      out_dir: directory to write the preprocessed files\n",
        "    Returns:\n",
        "      the number of (context, question, answer) triples written to file by the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    num_exs = 0 # number of examples written to file\n",
        "    num_mappingprob, num_tokenprob, num_spanalignprob = 0, 0, 0\n",
        "    examples = []\n",
        "\n",
        "    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(tier)):\n",
        "\n",
        "        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
        "        for pid in range(len(article_paragraphs)):\n",
        "\n",
        "            context = unicode(article_paragraphs[pid]['context']) # string\n",
        "\n",
        "            # The following replacements are suggested in the paper\n",
        "            # BidAF (Seo et al., 2016)\n",
        "            context = context.replace(\"''\", '\" ')\n",
        "            context = context.replace(\"``\", '\" ')\n",
        "\n",
        "            context_tokens = tokenize(context) # list of strings (lowercase)\n",
        "            context = context.lower()\n",
        "\n",
        "            qas = article_paragraphs[pid]['qas'] # list of questions\n",
        "\n",
        "            charloc2wordloc = get_char_word_loc_mapping(context, context_tokens) # charloc2wordloc maps the character location (int) of a context token to a pair giving (word (string), word loc (int)) of that token\n",
        "\n",
        "            if charloc2wordloc is None: # there was a problem\n",
        "                num_mappingprob += len(qas)\n",
        "                continue # skip this context example\n",
        "\n",
        "            # for each question, process the question and answer and write to file\n",
        "            for qn in qas:\n",
        "\n",
        "                # read the question text and tokenize\n",
        "                question = unicode(qn['question']) # string\n",
        "                question_tokens = tokenize(question) # list of strings\n",
        "\n",
        "                # of the three answers, just take the first\n",
        "                ans_text = unicode(qn['answers'][0]['text']).lower() # get the answer text\n",
        "                ans_start_charloc = qn['answers'][0]['answer_start'] # answer start loc (character count)\n",
        "                ans_end_charloc = ans_start_charloc + len(ans_text) # answer end loc (character count) (exclusive)\n",
        "\n",
        "                # Check that the provided character spans match the provided answer text\n",
        "                if context[ans_start_charloc:ans_end_charloc] != ans_text:\n",
        "                  # Sometimes this is misaligned, mostly because \"narrow builds\" of Python 2 interpret certain Unicode characters to have length 2 https://stackoverflow.com/questions/29109944/python-returns-length-of-2-for-single-unicode-character-string\n",
        "                  # We should upgrade to Python 3 next year!\n",
        "                  num_spanalignprob += 1\n",
        "                  continue\n",
        "\n",
        "                # get word locs for answer start and end (inclusive)\n",
        "                ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
        "                ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
        "                assert ans_start_wordloc <= ans_end_wordloc\n",
        "\n",
        "                # Check retrieved answer tokens match the provided answer text.\n",
        "                # Sometimes they won't match, e.g. if the context contains the phrase \"fifth-generation\"\n",
        "                # and the answer character span is around \"generation\",\n",
        "                # but the tokenizer regards \"fifth-generation\" as a single token.\n",
        "                # Then ans_tokens has \"fifth-generation\" but the ans_text is \"generation\", which doesn't match.\n",
        "                ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n",
        "                if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n",
        "                    num_tokenprob += 1\n",
        "                    continue # skip this question/answer pair\n",
        "\n",
        "                examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)])))\n",
        "\n",
        "                num_exs += 1\n",
        "\n",
        "    print(\"Number of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob)\n",
        "    print(\"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob)\n",
        "    print(\"Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems): \", num_spanalignprob)\n",
        "    print(\"Processed %i examples of total %i\\n\" % (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob))\n",
        "\n",
        "    # shuffle examples\n",
        "    indices = range(len(examples))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    with open(os.path.join(out_dir, tier +'.context'), 'w') as context_file,  \\\n",
        "         open(os.path.join(out_dir, tier +'.question'), 'w') as question_file,\\\n",
        "         open(os.path.join(out_dir, tier +'.answer'), 'w') as ans_text_file, \\\n",
        "         open(os.path.join(out_dir, tier +'.span'), 'w') as span_file:\n",
        "\n",
        "        for i in indices:\n",
        "            (context, question, answer, answer_span) = examples[i]\n",
        "\n",
        "            # write tokenized data to file\n",
        "            write_to_file(context_file, context)\n",
        "            write_to_file(question_file, question)\n",
        "            write_to_file(ans_text_file, answer)\n",
        "            write_to_file(span_file, answer_span)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-987e20bff34e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefaultencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'sys' has no attribute 'setdefaultencoding'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ1mTfuCaigP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    args = setup_args()\n",
        "\n",
        "    print(\"Will download SQuAD datasets to {}\".format(args.data_dir))\n",
        "    print(\"Will put preprocessed SQuAD datasets in {}\".format(args.data_dir))\n",
        "\n",
        "    if not os.path.exists(args.data_dir):\n",
        "        os.makedirs(args.data_dir)\n",
        "\n",
        "    train_filename = \"/content/drive/My Drive/Colab Notebooks/train-squad-v2.0 (1).json\"\n",
        "    dev_filename = \"/content/drive/My Drive/Colab Notebooks/dev-squad-v2.0.json\"\n",
        "\n",
        "    # download train set\n",
        "    maybe_download(SQUAD_BASE_URL, train_filename, args.data_dir, 30288272L)\n",
        "\n",
        "    # read train set\n",
        "    train_data = data_from_json(os.path.join(args.data_dir, train_filename))\n",
        "    print \"Train data has %i examples total\" % total_exs(train_data)\n",
        "\n",
        "    # preprocess train set and write to file\n",
        "    preprocess_and_write(train_data, 'train', args.data_dir)\n",
        "\n",
        "    # download dev set\n",
        "    maybe_download(SQUAD_BASE_URL, dev_filename, args.data_dir, 4854279L)\n",
        "\n",
        "    # read dev set\n",
        "    dev_data = data_from_json(os.path.join(args.data_dir, dev_filename))\n",
        "    print \"Dev data has %i examples total\" % total_exs(dev_data)\n",
        "\n",
        "    # preprocess dev set and write to file\n",
        "    preprocess_and_write(dev_data, 'dev', args.data_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}